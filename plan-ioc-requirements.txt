I would like to begin work on a new project called BURST (BTRFS Ultrafast Restore From S3 Transfers).
The project's main goal will be to transform an archive file in S3 object storage to a file tree on
a BTRFS filesystem as quickly as technically possible.

There are at least three key ideas to leverage in achieving this goal:
 1. Utilize best available S3 download performance optimizations. These include:
    a) S3 read performance scales nearly linearly with number of ranged requests sent concurrently for
    parts of the object, especially when those ranged requests are aligned to 8 or 16 MiB offsets
    from the beginning of the object and when parts with identically aligned boundaries were originally
    uploaded to S3 using the multipart upload API.
    b) Other micro-optimizations, especially when running on EC2 instances (which is our objective) are
    captured by the C S3 client at https://github.com/awslabs/aws-c-s3/. We should use this client.
 2. BTRFS supports an ioctl called BTRFS_IOC_ENCODED_WRITE which allows one to write pre-compressed data
    to the filesystem as (possibly part of) the content of a file. BTRFS has certain restrictions on what
    kind of compression algorithms and settings it supports, but if you comply with these restrictions,
    you can write compressed data directly to disk that is decompressed by BTRFS on read. We want to use
    this idea to compress the data in S3 identically to how it will be compressed after being restored on
    disk. In doing so, we gain performance benefits of less network transfer and less disk write bandwidth
    required, and avoid bottlnecking at the CPU to compress the data for BTRFS.

    We will use Zstandard as the compression algorithm because of its high performance and support by BTRFS.
 3. While concurrently transferring parts from S3, avoid needing to buffer large amounts of data in memory.
    Instead, utilize the features of zip archive headers/directories and zstandard skippable frames so that, after reading
    the zip file's central directory at its end, we are able to know where to write the data to disk at the beginning of
    every 8 MiB'th part of the S3 object.

    By doing this, we can simultaneously receive data and write it to its final home on disk, instead of doing
    these two operations sequentially. We also are more memory efficient.

A secondary, but still important, goal of the project will be that its archive files saved to S3 should be able
to be interpreted by any regular zip archive extraction software that supports Zstandard compression. (Note that
there are few such implementations currently; we can use [7-Zip-zstd](https://github.com/mcmilk/7-Zip-zstd) to
verify this goal is being achieved.)

Consequently, the project will require creating three primary deliverable artifacts:
 1. A full specification of an archive format that:
    a) Is supported by zip extractors that support Zstandard
    b) Limits its use of zstandard to the subset of frame sizes, compression levels, etc. that BTRFS supports
       for transparent decompression on read. This includes Zstandard frame sizes that do not exceed 128KiB,
       and compression levels must be between -15 through 15.
    c) To avoid buffering downloaded data when concurrently transferring each 8MiB part, arranges for it to be
       possible to know, having read the zip file's central directory, the details of the BTRFS_IOC_ENCODED_WRITE
       ioctl to issue corresponding to the data at the begining of each 8MiB part.

 2. An archive writer software that transforms filesystem trees on mounted filesystems into archive byte streams
    compliant with the specification. (Note, the writer need not exhibit optimizations like concurrency. Simply
    outputting the compliant archive file byte stream to an unseekable stream writer is sufficient.)

 3. An archive downloader and writer that uses https://github.com/awslabs/aws-c-s3/ to obtain archive byte streams
    from S3 concurrently in parts and issue BTRFS_IOC_ENCODED_WRITE calls to load the downloaded data onto the filesystem.

# Phase 1

This phase is complete. We researched and documented the requirements BTRFS has for using BTRFS_IOC_ENCODED_WRITE,
so that we could make informed decisions about how to design the archive format and the archive writer and downloader software.

You can read the findings from this research at BTRFS_IOC_ENCODED_WRITE-findings.md.

# Phase 2

Now let's use this understanding to specifically design the archive format. At the end of this phase, we should know how
the downloader/extractor software will know how to begin providing BTRFS_IOC_ENCODED_WRITE calls for each 8MiB part of the S3 object,
after having read the zip central directory at the end of the S3 object.

A key problem is that if the beginning of any 8MiB part of the archive could be anything (such as the middle of a zstandard frame),
then it would be necessary to determine where the next zstandard frame begins, what uncompressed offset that next frame should be written to,
and we would need to buffer and use later the data between the beginning of the 8MiB part and the beginning of the next zstandard frame.

We should not entirely discount an approach that favors increased complexity of the downloader/extractor to solve these problems. Let's
investigate how best to accomplish this.

Alternatively, we could design the archive format so that the beginning of each 8MiB part is always aligned to the beginning of a zstandard frame
or beginning of a file entry in the zip archive. This would simplify the downloader/extractor, but would likely complicate the archive writer.
